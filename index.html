<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sucheng Ren</title>

  <meta name="author" content="Sucheng Ren">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:110%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sucheng (Oliver) Ren</name>
                  </p>
                  <p>
                    I am a master student advised by <a href="http://www.shengfenghe.com/">Shengfeng He</a> at the South China University of Technology, where I received my B.S. degree. The expecting date to get my M.S. degree in Computer Science is June 2022. I am interested in Transformer, Self-Supervised Learning, Knowledge Distillation, and Multimodal Learning.
                  </p>
                  <p>
                    I am very lucky to work with Prof. <a href="https://cihangxie.github.io/">Cihang Xie</a> and Prof. <a href="http://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> on Self-Supervised Learning at John Hopkins University, 
                    <a href="https://scholar.google.com.hk/citations?user=Q8iay0gAAAAJ&hl=zh-CN">Jiashi Feng</a> on Transformer at National University of Singapore, and Prof. <a href="http://people.csail.mit.edu/hangzhao/">Hang Zhao</a> on Multi-modal Learning and Knowledge Distillation at Tsinghua University. 
                    
                  </p>
                  <p>
                    <font color='red'>I am looking for positions for PhD in 2022 Fall. If you have available positions, please
                    contact me. I am always ready to have a chat.üòÄ</font>
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:oliverrensu@gmail.com">Email</a> &nbsp|&nbsp
                    <a href="data/Sucheng_Ren_cv.pdf">CV</a> &nbsp|&nbsp
                    <a href="https://scholar.google.com/citations?user=Hbf-SoAAAAAJ&hl=zh-CN">Scholar</a> &nbsp|&nbsp
                    <a href="https://github.com/OliverRensu">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/profile.jpg"><img style="width:110%;max-width:110%" alt="profile photo"
                      src="images/profile.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>



          <table width="110%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="110%" align="center" border="0" cellpadding="20">
            <tbody>
              <ul>
                <Li>[Feb 2022] Three first author papers are accepted by CVPR2022!üéâ</li>>
                <li>[Sep 2021] Working with Prof. <a href="https://scholar.google.com.hk/citations?user=Q8iay0gAAAAJ&hl=zh-CN">Jiashi Feng</a> at National University of Singapore as a research intern!üë©‚Äçüíª</li>
                <li>[Jul 2021] Two papers are accepted by ICCV2021  including one oral paper (Acceptance Rate 3.0%)!üéâ</li>
                <li>[May 2021] Working with Prof. <a href="https://cihangxie.github.io/">Cihang Xie</a> and Prof. <a href="http://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> at John Hopkins University!üë©‚Äçüíª</li>
                <li>[Mar 2021] Three papers are accepted by CVPR2021 including two first author papers!üéâ</li>
                <li>[Dec 2020] Going to work with Prof. Hang Zhao in Tsinghua as a research intern!üë©‚Äçüíª</li>
                <li>[Jul 2020] A first author paper is accepted by ECCV2020 as spotlight paper (Acceptance Rate 5.0%)!üéâ</li>
              </ul>
            </tbody>
          </table>



          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:110%;vertical-align:middle">
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:25%;vertical-align:left">
                  <div><img src="images/new_MKE.png"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>Multimodal Knowledge Expansion                      
                    </papertitle>
                  <br>
                  <br>
                  Zihui Xue, 
                  <strong>Sucheng Ren</strong>, 
                  <a href="https://zhengqigao.github.io/">Zhengqi Gao</a>, 
                  <a href="http://people.csail.mit.edu/hangzhao/">Hang Zhao</a>

                  <br>
                  International Conference on Computer Vision <strong>(ICCV)</strong>, 2021
                  <br>
                  <a href="https://arxiv.org/abs/2103.14431">[paper]</a>
                  
                  <a href="data/MKE.bib">[bibtex]</a>
                  <p>
                    A knowledge distillation-based framework to effectively utilize multimodal data without requiring labels.
                  </p>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:25%;vertical-align:left">
                  <div><img src="images/new_decorr.png"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>On Feature Decorrelation in Self-Supervised Learning                      
                    </papertitle>
                  <br>
                  <br>
                  <a href="https://patrickhua.github.io/">Tianyu Hua</a>,
                  <a href="https://scholar.google.com/citations?user=hn0u5VgAAAAJ&hl=zh-CN">Wenxiao Wang</a>, 
                  Zihui Xue, 
                  <strong>Sucheng Ren</strong>, 
                  <a href="https://people.csail.mit.edu/yuewang/">Yue Wang</a>, 
                  <a href="http://people.csail.mit.edu/hangzhao/">Hang Zhao</a>
                  

                  <br>
                  International Conference on Computer Vision <strong>(ICCV)</strong>, 2021
                  <br>
                  (<strong>Oral</strong>, Acceptance Rate 3.0%)
                  <br>
                  <a href="https://arxiv.org/abs/2105.00470">[paper]</a>
                  
                  <a href="data/Decorr.bib">[bibtex]</a>
                  <p>
                    Connecting dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation.
                  </p>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:20px;width:25%;vertical-align:left">
                  <div><img src="images/lipreading.png"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>Learning from the Master: Distilling Cross-modal Advanced Knowledge for Lip Reading                      
                    </papertitle>
                  <br>
                  <strong>Sucheng Ren, </strong>
                  Yong Du,
                  Jianming Lv,
                  Guoqiang Han, 
                  and <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Learning_From_the_Master_Distilling_Cross-Modal_Advanced_Knowledge_for_Lip_CVPR_2021_paper.pdf">[paper]</a>
                  <!--<a href="https://arxiv.org/pdf/2012.01050.pdf">[paper]</a>-->
                  <a href="data/lipreading.bib">[bibtex]</a>
                  <p>
                    Training a master to learn how to teach a better student.
                  </p>
                </td>
              </tr>
          </table>

          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:24px;width:25%;vertical-align:left">
                  <div><img src="images/RTM.png"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>Reciprocal Transformations for Unsupervised Video Object Segmentation
                    </papertitle>
                  </a>
                  <br>
                  <br>
                  <strong>Sucheng Ren, </strong>
                  Wenxi Liu,
                  Yongtuo Liu,
                  Haoxin Chen,
                  Guoqiang Han and
                  <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Reciprocal_Transformations_for_Unsupervised_Video_Object_Segmentation_CVPR_2021_paper.pdf">[paper]</a>
                  <!--<a href="https://arxiv.org/pdf/2012.01050.pdf">[paper]</a>-->
                  <a href="data/RTM.bib">[bibtex]</a>
                  <a href="https://github.com/OliverRensu/RTNet">[code]</a>
                  <p>
                    Jointly learning salient objects, moving objects, recurring objects for Unsupervised Video Object Segmentation.
                  </p>
                </td>
              </tr>
          </table>
          
          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:28px;width:25%;vertical-align:left">
                  <div><img src="images/FVOS.png"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>Delving Deep into Many-to-many Attention forFew-shot Video Object Segmentation
                    </papertitle>
                  </a>
                  <br>
                  <br>
                  Haoxin Chen, Hanjie Wu, Nanxuan Zhao, <strong>Sucheng Ren</strong> and
                  <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong></em>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Delving_Deep_Into_Many-to-Many_Attention_for_Few-Shot_Video_Object_Segmentation_CVPR_2021_paper.pdf">[paper]</a>
                  <a href="data/DANet.bib">[bibtex]</a>
                  <a href="https://github.com/scutpaul/DANet">[code]</a>
                  <p>
                    
                  </p>
                </td>
              </tr>
          </table>
          
          <table
            style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                <td style="padding:24px;width:25%;vertical-align:left">
                  <div><img src="images/TENET.png"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>TENet: Triple Excitation Network for Video Salient Object Detection
                    </papertitle>
                  </a>
                  <br>
                  <br>
                  <strong>Sucheng Ren,</strong>
                  Chu Han,
                  Xin Yang,
                  Guoqiang Han and
                  <a href="http://www.shengfenghe.com">Shengfeng He</a>

                  <br>
                  European Conference on Computer Vision <strong>(ECCV)</strong>, 2020
                  <br>
                  (<strong>Spotlight</strong>, Acceptance Rate 5.0%)
                  <br>
                  <a href="https://arxiv.org/abs/2007.09943">[paper]</a>
                  <a href="data/TENet.bib">[bibtex]</a>
                  <p>
                    
                  </p>
                </td>
              </tr>
          </table>

          



  <!-- Default Statcounter code for Personal Website
https://oliverrensu.github.io/ -->
<script type="text/javascript">
  var sc_project=12694920; 
  var sc_invisible=1; 
  var sc_security="f1698122"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics Made Easy -
  Statcounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/12694920/0/f1698122/1/"
  alt="Web Analytics Made Easy - Statcounter"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->
</body>
</html>